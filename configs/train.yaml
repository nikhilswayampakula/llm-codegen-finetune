model_name: gpt2
output_dir: models/lora-gpt2
train_file: data/datasets/code_pairs.jsonl
eval_ratio: 0.02
block_size: 256
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 2.0e-4
num_train_epochs: 1
lora:
  r: 16
  alpha: 32
  dropout: 0.05
seed: 42
