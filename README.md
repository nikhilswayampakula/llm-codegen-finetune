# ğŸ§  Fine-Tuning LLMs Using Real-Time Code Files from GitHub to Build a Code Generation Model

A production-grade pipeline to **ingest real-time GitHub code**, curate a clean dataset, and **fineâ€‘tune a code LLM** with **LoRA/PEFT** for **completion & generation**.  
Includes **FastAPI** inference server, **Streamlit** playground, tests, CI, Docker, and MIT license.

## ğŸ”§ Features
- ğŸ”„ **Ingest** public repos (URLs list) â†’ filter languages, de-duplicate, strip boilerplate
- ğŸ§¹ **Curate** to JSONL pairs *(prompt â†’ completion)* suitable for causal LM SFT
- ğŸ‹ï¸ **Train** LoRA on HuggingFace causal models (`gpt2`, `TinyLlama`, etc.) via **Transformers + PEFT + Accelerate**
- ğŸš€ **Serve** completions via **FastAPI** + interactive **Streamlit** UI
- ğŸ§° **DevOps**: Dockerfile, Makefile, GitHub Actions CI, `.gitkeep`

## âš¡ Quickstart
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# 1) Ingest + build dataset
python src/ingest_github.py --urls data/raw/sample_repos.txt --out data/processed
python src/build_dataset.py --src data/processed --out data/datasets/code_pairs.jsonl

# 2) Train LoRA (small model for demo)
python src/train_lora.py --config configs/train.yaml

# 3) Serve API and UI
uvicorn app.api:app --reload --port 8000
streamlit run app/streamlit_app.py
```

## ğŸ“¡ API
- `GET /health` â†’ status + model loaded flag  
- `POST /generate` â†’ JSON `{"prompt": "def add(a, b):"}` â†’ returns completion

## ğŸ“‚ Structure
```
llm-codegen-finetune/
â”œâ”€â”€ app/ (FastAPI + Streamlit)
â”œâ”€â”€ configs/
â”œâ”€â”€ data/ (raw/ processed/ datasets/)
â”œâ”€â”€ models/ (saved LoRA)
â”œâ”€â”€ src/ (pipeline + training + infer)
â”œâ”€â”€ tests/
â””â”€â”€ DevOps: Dockerfile, Makefile, CI
```

**Author:** Nikhil Swayampakula Â· LinkedIn: https://www.linkedin.com/in/nikhil-swa-47b479366/ Â· **License:** MIT
